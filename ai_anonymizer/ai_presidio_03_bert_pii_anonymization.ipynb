{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install presidio-analyzer\n",
        "# !pip install presidio-anonymizer\n",
        "# !pip install sentencepiece transformers\n",
        "# !python -m spacy download sv_core_news_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- https://github.com/abderrahmane-mhd/text-anonymization/blob/main/TextAnonymization.ipynb\n",
        "- https://www.philschmid.de/pii-huggingface-sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvzX080RRN2Y"
      },
      "source": [
        "# Swedish Text Anonymization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CHOZ5qIVffk3"
      },
      "outputs": [],
      "source": [
        "document_content = \"\"\"Ämne: Förfrågan om Byte av Adressuppgifter\n",
        "\n",
        "Hej,\n",
        "\n",
        "Mitt namn är Anna Svensson och jag skriver till er för att uppdatera\n",
        "mina personliga uppgifter i era register.\n",
        "Nyligen har jag flyttat till en ny adress och det är viktigt för mig\n",
        "att mina uppgifter är aktuella hos er.\n",
        "\n",
        "Här är mina uppdaterade uppgifter:\n",
        "\n",
        "Namn: Anna Svensson\n",
        "Personnummer: 860711-1234\n",
        "Gammal adress: Storgatan 12, 123 45 Gamlastaden\n",
        "Ny adress: Lillgatan 34, 543 21 Nystaden\n",
        "Telefonnummer: 070-1234567\n",
        "E-post: anna.svensson@email.com\n",
        "\n",
        "Tack på förhand för er hjälp med denna ärende.\n",
        "\n",
        "Med vänliga hälsningar,\n",
        "Anna Svensson\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-27 21:04:13.948106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-27 21:04:16.042460: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-01-27 21:04:16.042722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-01-27 21:04:16.042730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2024-01-27 21:04:17.885822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2024-01-27 21:04:17.886042: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-01-27 21:04:17.886098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BB904106): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult, Pattern, PatternRecognizer\n",
        "\n",
        "from presidio_analyzer.nlp_engine import NlpArtifacts,NlpEngineProvider\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_colored_pii(string):\n",
        "    colored_string = re.sub(\n",
        "        r\"(<[^>]*>)\", lambda m: \"\\033[31m\" + m.group(1) + \"\\033[0m\", string\n",
        "    )\n",
        "    print(colored_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obOYQYOLiMiH"
      },
      "source": [
        "## 1. NER SpaCy Model\n",
        "\n",
        "We will start by trying NER model proposed by SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "weU3QnrUiP_Z"
      },
      "outputs": [],
      "source": [
        "lang_code = \"sv\"\n",
        "model_name = \"sv_core_news_lg\"\n",
        "nlp = spacy.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tGLP7z2PK5B1"
      },
      "outputs": [],
      "source": [
        "configuration = {\"nlp_engine_name\":\"spacy\",\n",
        "                 \"models\":[{\n",
        "                     \"lang_code\": lang_code,\n",
        "                     \"model_name\":model_name\n",
        "                     }]\n",
        "                }\n",
        "\n",
        "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
        "\n",
        "nlp_engine = provider.create_engine()\n",
        "\n",
        "analyzer = AnalyzerEngine(\n",
        "    nlp_engine=nlp_engine,\n",
        "    supported_languages = [lang_code]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0647MAbLSnA",
        "outputId": "45f8ca38-91e4-41ae-a0cc-e102e119e940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type: EMAIL_ADDRESS, start: 476, end: 499, score: 1.0\n",
            "type: LOCATION, start: 367, end: 376, score: 0.85\n",
            "type: LOCATION, start: 411, end: 420, score: 0.85\n",
            "type: URL, start: 476, end: 483, score: 0.5\n",
            "type: URL, start: 490, end: 499, score: 0.5\n",
            "type: PHONE_NUMBER, start: 340, end: 351, score: 0.4\n"
          ]
        }
      ],
      "source": [
        "results = analyzer.analyze(text=document_content, language=lang_code)\n",
        "\n",
        "for res in results:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iEgepz0LcCK",
        "outputId": "1839ac73-78bd-47a8-85c8-39b89fc41626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'anna.sv', 'anna.svensson@email.com', '860711-1234', 'Storgatan', 'email.com', 'Lillgatan'}\n"
          ]
        }
      ],
      "source": [
        "found_entities = [document_content[obj.to_dict()['start']:obj.to_dict()['end']] for obj in results]\n",
        "\n",
        "spacy_detected_entities = set(found_entities)\n",
        "\n",
        "print(spacy_detected_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ämne: Förfrågan om Byte av Adressuppgifter\n",
            "\n",
            "Hej,\n",
            "\n",
            "Mitt namn är Anna Svensson och jag skriver till er för att uppdatera\n",
            "mina personliga uppgifter i era register.\n",
            "Nyligen har jag flyttat till en ny adress och det är viktigt för mig\n",
            "att mina uppgifter är aktuella hos er.\n",
            "\n",
            "Här är mina uppdaterade uppgifter:\n",
            "\n",
            "Namn: Anna Svensson\n",
            "Personnummer: \u001b[31m<PHONE_NUMBER>\u001b[0m\n",
            "Gammal adress: \u001b[31m<LOCATION>\u001b[0m 12, 123 45 Gamlastaden\n",
            "Ny adress: \u001b[31m<LOCATION>\u001b[0m 34, 543 21 Nystaden\n",
            "Telefonnummer: 070-1234567\n",
            "E-post: \u001b[31m<EMAIL_ADDRESS>\u001b[0m\n",
            "\n",
            "Tack på förhand för er hjälp med denna ärende.\n",
            "\n",
            "Med vänliga hälsningar,\n",
            "Anna Svensson\n"
          ]
        }
      ],
      "source": [
        "# the analyzer results are passed to the AnonymizerEngine for redaction(anonymization)\n",
        "anonymizer = AnonymizerEngine()\n",
        "anonymized_text = anonymizer.anonymize(text=document_content, analyzer_results=results)\n",
        "\n",
        "print_colored_pii(anonymized_text.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQwYjmQwytGM"
      },
      "source": [
        "## 2. NER Transformers Model\n",
        "\n",
        "In this section we try out a Name Entity Recognition Model using a BERT transformer model for Swedish Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DgnKOudFGP4k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Loading both tokenizer and NER model\n",
        "model_path = 'KBLab/bert-base-swedish-cased-ner'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CpASFtAeKB-4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline('ner', model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "transformer_res = nlp(document_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.999498,\n",
              "  'word': 'Anna Svensson',\n",
              "  'start': 63,\n",
              "  'end': 76},\n",
              " {'entity_group': 'TME',\n",
              "  'score': 0.99970055,\n",
              "  'word': 'Nyligen',\n",
              "  'start': 161,\n",
              "  'end': 168},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.9997503,\n",
              "  'word': 'Anna Svensson',\n",
              "  'start': 312,\n",
              "  'end': 325},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9978662,\n",
              "  'word': 'Storgatan 12',\n",
              "  'start': 367,\n",
              "  'end': 379},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9884695,\n",
              "  'word': 'Gamlastaden',\n",
              "  'start': 388,\n",
              "  'end': 399},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9973336,\n",
              "  'word': 'Lillgatan 34',\n",
              "  'start': 411,\n",
              "  'end': 423},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.5679079,\n",
              "  'word': '543',\n",
              "  'start': 425,\n",
              "  'end': 428},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9952892,\n",
              "  'word': 'Nystaden',\n",
              "  'start': 432,\n",
              "  'end': 440},\n",
              " {'entity_group': 'MSR',\n",
              "  'score': 0.8682774,\n",
              "  'word': '070',\n",
              "  'start': 456,\n",
              "  'end': 459},\n",
              " {'entity_group': 'MSR',\n",
              "  'score': 0.5759755,\n",
              "  'word': '##45',\n",
              "  'start': 463,\n",
              "  'end': 465},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.9998516,\n",
              "  'word': 'Anna Svensson',\n",
              "  'start': 573,\n",
              "  'end': 586}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "paFg-PsUKPWX"
      },
      "outputs": [],
      "source": [
        "bert_detected_entities = [res['word'] for res in transformer_res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0eSIsEKnka",
        "outputId": "af4979d1-82e1-4532-edd7-a6463bd63fb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'##45',\n",
              " '070',\n",
              " '543',\n",
              " 'Anna Svensson',\n",
              " 'Gamlastaden',\n",
              " 'Lillgatan 34',\n",
              " 'Nyligen',\n",
              " 'Nystaden',\n",
              " 'Storgatan 12'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(bert_detected_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-M3WIMPTL_2"
      },
      "source": [
        "## 3. Mixed Pipeline development (Transformers + SpaCy)\n",
        "\n",
        "- https://microsoft.github.io/presidio/samples/python/transformers_recognizer/\n",
        "- https://microsoft.github.io/presidio/analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "mapping_labels = {\"PER\":\"PERSON\",'LOC':'LOCATION','ORG':\"ORGANIZATION\",'PHONE_NUMBER':'PHONE_NUMBER'}\n",
        "\n",
        "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
        "DEFAULT_ANOYNM_ENTITIES = [\n",
        "    \"CREDIT_CARD\",\n",
        "    \"CRYPTO\",\n",
        "    \"DATE_TIME\",\n",
        "    \"EMAIL_ADDRESS\",\n",
        "    \"IBAN_CODE\",\n",
        "    \"IP_ADDRESS\",\n",
        "    \"NRP\",\n",
        "    \"LOCATION\",\n",
        "    \"PERSON\",\n",
        "    \"PHONE_NUMBER\",\n",
        "    \"MEDICAL_LICENSE\",\n",
        "    \"URL\",\n",
        "    \"ORGANIZATION\",\n",
        "    \"NUMBER\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hgPuExQ7GN7B"
      },
      "outputs": [],
      "source": [
        "class HFTransformerRecognizer(EntityRecognizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id_or_path,\n",
        "        mapping_labels,\n",
        "        aggregation_strategy=\"simple\",\n",
        "        supported_language=lang_code,\n",
        "        ignore_labels=[\"O\", \"MISC\"],\n",
        "    ):\n",
        "        # inits transformers pipeline for given mode or path\n",
        "        self.pipeline = pipeline(\n",
        "            \"token-classification\", model=model_id_or_path, aggregation_strategy=aggregation_strategy, ignore_labels=ignore_labels\n",
        "        )\n",
        "        # map labels to presidio labels\n",
        "        self.label2presidio = mapping_labels\n",
        "\n",
        "        # passes entities from model into parent class\n",
        "        super().__init__(supported_entities=list(self.label2presidio.values()), supported_language=supported_language)\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"No loading is required.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def analyze(\n",
        "        self, text: str, entities = None, nlp_artifacts: NlpArtifacts = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Extracts entities using Transformers pipeline\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        predicted_entities = self.pipeline(text)\n",
        "        if len(predicted_entities) > 0:\n",
        "            for e in predicted_entities:\n",
        "                if(e['entity_group'] not in self.label2presidio):\n",
        "                    continue\n",
        "                converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
        "                if converted_entity in entities or entities is None:\n",
        "                    results.append(\n",
        "                        RecognizerResult(\n",
        "                            entity_type=converted_entity, start=e[\"start\"], end=e[\"end\"], score=e[\"score\"]\n",
        "                        )\n",
        "                    )\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "80SbWEtDHGBa"
      },
      "outputs": [],
      "source": [
        "to_keep = []\n",
        "configuration = {\n",
        "    \"nlp_engine_name\": \"spacy\",\n",
        "    \"models\": [{\"lang_code\": lang_code, \"model_name\": model_name}],\n",
        "}\n",
        "\n",
        "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
        "\n",
        "nlp_engine = provider.create_engine()\n",
        "\n",
        "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
        "analyzer = AnalyzerEngine(\n",
        "    nlp_engine=nlp_engine,\n",
        "    supported_languages = [lang_code]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xe2qcwhaGbI7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "transformers_recognizer = HFTransformerRecognizer(model_path, mapping_labels)\n",
        "analyzer.registry.add_recognizer(transformers_recognizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpJyGgN3OvCZ",
        "outputId": "5af5165b-aed3-4c6c-be58-f4fed7442946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# Text Analyzer\n",
        "analyzer_results = analyzer.analyze(text=document_content, entities = DEFAULT_ANOYNM_ENTITIES, allow_list = to_keep, language=lang_code)\n",
        "\n",
        "# Text Anonymizer\n",
        "anonymizer = AnonymizerEngine()\n",
        "anonymized_text = anonymizer.anonymize(text=document_content, analyzer_results=analyzer_results)\n",
        "\n",
        "# Restructuring anonymizer results\n",
        "\n",
        "anonymization_results =  {\"anonymized\": anonymized_text.text,\"found\": [entity.to_dict() for entity in analyzer_results]}\n",
        "\n",
        "words = [{'word': document_content[obj['start']:obj['end']], 'entity_type':obj['entity_type'], 'start':obj['start'], 'end':obj['end']} for obj in anonymization_results['found']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gkqt7clQSvF",
        "outputId": "ec09da76-4702-478c-bb39-47d1ac479ff6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'anna.svensson@email.com',\n",
              "  'entity_type': 'EMAIL_ADDRESS',\n",
              "  'start': 476,\n",
              "  'end': 499},\n",
              " {'word': 'Anna Svensson', 'entity_type': 'PERSON', 'start': 573, 'end': 586},\n",
              " {'word': 'Anna Svensson', 'entity_type': 'PERSON', 'start': 312, 'end': 325},\n",
              " {'word': 'Anna Svensson', 'entity_type': 'PERSON', 'start': 63, 'end': 76},\n",
              " {'word': 'Storgatan 12', 'entity_type': 'LOCATION', 'start': 367, 'end': 379},\n",
              " {'word': 'Lillgatan 34', 'entity_type': 'LOCATION', 'start': 411, 'end': 423},\n",
              " {'word': 'Nystaden', 'entity_type': 'LOCATION', 'start': 432, 'end': 440},\n",
              " {'word': 'Gamlastaden', 'entity_type': 'LOCATION', 'start': 388, 'end': 399},\n",
              " {'word': '543', 'entity_type': 'LOCATION', 'start': 425, 'end': 428},\n",
              " {'word': 'anna.sv', 'entity_type': 'URL', 'start': 476, 'end': 483},\n",
              " {'word': 'email.com', 'entity_type': 'URL', 'start': 490, 'end': 499},\n",
              " {'word': '860711-1234',\n",
              "  'entity_type': 'PHONE_NUMBER',\n",
              "  'start': 340,\n",
              "  'end': 351}]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ämne: Förfrågan om Byte av Adressuppgifter\n",
            "\n",
            "Hej,\n",
            "\n",
            "Mitt namn är \u001b[31m<PERSON>\u001b[0m och jag skriver till er för att uppdatera\n",
            "mina personliga uppgifter i era register.\n",
            "Nyligen har jag flyttat till en ny adress och det är viktigt för mig\n",
            "att mina uppgifter är aktuella hos er.\n",
            "\n",
            "Här är mina uppdaterade uppgifter:\n",
            "\n",
            "Namn: \u001b[31m<PERSON>\u001b[0m\n",
            "Personnummer: \u001b[31m<PHONE_NUMBER>\u001b[0m\n",
            "Gammal adress: \u001b[31m<LOCATION>\u001b[0m, 123 45 \u001b[31m<LOCATION>\u001b[0m\n",
            "Ny adress: \u001b[31m<LOCATION>\u001b[0m, \u001b[31m<LOCATION>\u001b[0m 21 \u001b[31m<LOCATION>\u001b[0m\n",
            "Telefonnummer: 070-1234567\n",
            "E-post: \u001b[31m<EMAIL_ADDRESS>\u001b[0m\n",
            "\n",
            "Tack på förhand för er hjälp med denna ärende.\n",
            "\n",
            "Med vänliga hälsningar,\n",
            "\u001b[31m<PERSON>\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "print_colored_pii(anonymized_text.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
